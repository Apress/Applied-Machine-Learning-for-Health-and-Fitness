{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Action Recognition\n",
    "\n",
    "[*Applied Machine Learning for Health and Fitness*](https://www.apress.com/9781484257715) by Kevin Ashley (Apress, 2020).\n",
    "\n",
    "[*Video Course*](http://ai-learning.vhx.tv) Need a deep dive? Watch my [*video course*](http://ai-learning.vhx.tv) that complements this book with additional examples and video-walkthroughs. \n",
    "\n",
    "[*Web Site*](http://activefitness.ai) for research and supplemental materials.\n",
    "\n",
    "![](images/ch9/fig_9-1.jpeg)\n",
    "\n",
    "## Background\n",
    "\n",
    "Our brain is a super-fast action recognition system that's hard to match. In terms of deep learning our brain routinely does many things to recognize actions, and it works *fast*! It may be years of evolution, the need to identify incoming danger or provide food: each of has a miraculously fast video action recognition engine that just works. Our brain is capable of *normalization* and *transformation* since we recognize actions regardless of viewpoint. Human brain is great at *classification*, telling us what's moving and how, and it can also *predict* what's coming next. Turns out, our knowledge of deep learning for action recognition is getting close, but it's not as good just yet. Especially, it becomes clear when generalizing movements: the brain is far ahead of neural nets in its ability to generalize. Although if data science keeps the same pace of evolution as in the last decade, perhaps AI may get closer to the human brain.\n",
    "\n",
    "![](images/ch9/fig_9-2.png)\n",
    "\n",
    "Recognizing actions from videos is key to many industries: sport, surveillance, robotics, health care and many others. For a practical sport data scientist or a coach, action recognition is part of the daily job: coach's eye and experience is trained to do movement analysis.\n",
    "\n",
    "In biomechanics, we use physical or classical mechanics models to describe movement. This approach worked for many years in sports science, but analytical methods are complicated and as history shows: although movements can be described with classical mechanics, deep learning methods can be more efficient and often demonstrate great precision. To illustrate this point: Kinetics dataset described in this chapter contains 400 activity classes with hundreds of sport activities recognition readily available. You can classify any of those activities with an average level computer, and you really don't need a PhD in biomechanics.\n",
    "\n",
    "![](images/ch9/fig_9-3.png)\n",
    "\n",
    "Kinetics dataset has 650,000 video clips, covering hundreds of human actions and totaling close to a terabyte of data. That looks like a lot, but for each sport it covers only a few basic moves. A human ski coach evaluating a skier can narrow it down to dozens of small movements and usually deals with multiple training routines.\n",
    "\n",
    "Video recognition has been traditionally tough for deep learning because it needs more compute power and storage than most other types of data: that's a lot of power and storage! In the recent years, video recognition methods, models and datasets made a significant progress to the point that they became practical for a sport scientist. As this chapter shows, these methods are also relatively easy to use with the tools, frameworks and models available today. In this chapter, we focus on practical use and methods for video action recognition. You don't need advanced hardware, but a GPU enabled computer is recommended. If you don't have that handy, using an online service like Microsoft Azure or Google Colab that offers free scaleable compute service for data science.\n",
    "\n",
    "## Video data\n",
    "\n",
    "> Cinematography is a writing with images in mouvement and with sounds. Robert Bresson, Notes on the Cinematographer\n",
    "\n",
    "Video classification has been an expensive task because of the need to deal with the video, and video is heavy. In this chapter you'll learn data structures for video, used across most of datasets and models for video recognition.\n",
    "\n",
    "A single image, or video frame can be represented as a 3D tensor: (width, height, color), color depth having three channels: RGB. A sequence of frames can be represented as a 4D tensor: (frame, width, height, color). For video classification, you will typically deal with sequences, or batches of frames, and the video is represented as a 4D or 5D tensor: (sample, frame, width, height, color).\n",
    "\n",
    "For example, to read a video into structures ready for deep learning, frameworks provide convenience methods, such as PyTorch's torchvision.io.read\\_video. In the following code snippet, the video is loaded as a 4D tensor 255 (frames) x 720 (height) x 1280 (width) x 3 (colors). Notice, that this method also loads audio, although we are not going to use it for action recognition:\n",
    "\n",
    "```python\n",
    "\n",
    "import torchvision.io\n",
    "\n",
    "video_file = 'media/surfing_cutback.mp4'\n",
    "\n",
    "video, audio, info = torchvision.io.read_video(video_file, pts_unit='sec')\n",
    "\n",
    "print(video.shape, audio.shape, info)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "Output:\n",
    "\n",
    "torch.Size([255, 720, 1280, 3]) torch.Size([2, 407552]) {'video_fps': 29.97002997002997, 'audio_fps': 48000}\n",
    "```\n",
    "\n",
    "This original video is obviously too big: in order to use it further we'll need to normalize it. This chapter provides sample code that normalizes the video.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "> *The relationship between space and time is a mysterious one.\\\n",
    "> *- Carreira, Zimmerman \"Quo Vadis? Action Recognition\"\n",
    "\n",
    "Quo Vadis is Latin means: \"where are you going?\". Although action recognition is achievable from a still frame, it works best when learning from temporal component as well as spatial information.\n",
    "\n",
    "![](images/ch9/fig_9-4.png)\n",
    "\n",
    "From the still frame it's not easy to tell whether the person is swimming or running. Perhaps, this ambiguity in action recognition prompted authors of research article on Kinetics video dataset and Two-Stream Inflated 3D ConvNet (I3D) architecture to name the article.\n",
    "\n",
    "Prior to Kinetics, Sports-1M used to be a breakthrough dataset, in authors' own words:\n",
    "\n",
    "> To obtain sufﬁcient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports.\n",
    ">\n",
    "> Andrej Karpathy et al, \"Large-scale Video Classiﬁcation with Convolutional Neural Networks\"\n",
    "\n",
    "Historically, deep learning for video recognition focused on activities that were easily available. What source of video data, can a data scientist use without the need to store terabytes of videos? YouTube comes very handy, as well as any other online video service! You'll notice that many of the action recognition datasets use online video services because those videos are typically indexed, can be readily retrieved and often have additional metadata that helps classifying entire videos or even segments. In fact, with massive online video repositories, storing billions of movements and deep learning, we are on the verge of revolution in movement recognition!\n",
    "\n",
    "Some well-known datasets for human video action sequences include:\n",
    "\n",
    "-   HMDB 51 is a set of 51 action categories, including facial, body movements and human interaction. This dataset contains some sport activities, but is limited to bike, fencing, baseball and a few others. Included in PyTorch: torchvision.datasets.HMDB51\n",
    "\n",
    "-   UCF 101 is used in many action recognition scenarios, including human-object interaction, body motion, playing musical instruments and sports. Included in PyTorch: torchvision.datasets.UCF101\n",
    "\n",
    "-   Kinetics is a large dataset of URL links to video clips that covers human action classes, including sports, human interaction etc. The dataset is available in different sizes: Kinetics 400, 600, 700 and is included in PyTorch: torchvision.datasets.Kinetics400\n",
    "\n",
    "## Models\n",
    "\n",
    "While video presents many challenges: computational cost, capturing both spatial and temporal action over long periods of time, it also presents unique opportunities in terms of designing data models. Over the last few years, researchers experimented with various approaches to video action recognition modelling. Methods that prove most effective so far, are using pretrained networks, fusing various streams of data from video, for example motion stream from optical flow and spatial pretrained context [e.g. https://arxiv.org/pdf/1406.2199.pdf].\n",
    "\n",
    "\n",
    "![Modern models use fusion of context streams: for example temporal and spatial for action recognition](images/ch9/fig_9-5.png)\n",
    "\n",
    "Some earlier methods tried experimenting and benchmarking model performance with various context streams, for example using different context resolutions with something authors creatively called a \"fovea\" stream in one research and the main feature learning stream.\n",
    "\n",
    "> Fovea -- a small depression in the retina of the eye where visual activity is the highest.\n",
    ">\n",
    "> Oxford Dictionary\n",
    "\n",
    "This area of deep learning is still under active research and we may still see state of the art models that outperform existing methods.\n",
    "\n",
    "## Video Classification QuickStart\n",
    "\n",
    "### Project 9-1. QuickStart Action Recognition\n",
    "\n",
    "This project provides a quick start for video classification: the goal is to have a practical sport data scientist quickly started on the human activity recognition. Before we start on this project, let's take a look at the list of human activities we can classify with minimal effort. I'll be using PyTorch here, because it provides video classification datasets and pretrained models out of the box. PyTorch computer vision module, torchvision, contains many models and datasets we can use in sports data science, including classification, semantic segmentation, object detection, person keypoint detection and video classification. Video classification models and datasets included with PyTorch, are what we'll be using for this task to get started quickly.\n",
    "\n",
    "In PyTorch video classification models are trained with Kinetics 400 dataset. Although not all of these human activities are sports related, I put together a helper in utils.kinetics, conveniently, it provides a list of sport related activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athletics - jumping\n",
      "\thigh jump\n",
      "\thurdling\n",
      "\tlong jump\n",
      "\tparkour\n",
      "\tpole vault\n",
      "\ttriple jump\n",
      "athletics - throwing + launching\n",
      "\tarchery\n",
      "\tcatching or throwing frisbee\n",
      "\tdisc golfing\n",
      "\thammer throw\n",
      "\tjavelin throw\n",
      "\tshot put\n",
      "\tthrowing axe\n",
      "\tthrowing ball\n",
      "\tthrowing discus\n",
      "ball sports\n",
      "\tbowling\n",
      "\tcatching or throwing baseball\n",
      "\tcatching or throwing softball\n",
      "\tdodgeball\n",
      "\tdribbling basketball\n",
      "\tdunking basketball\n",
      "\tgolf chipping\n",
      "\tgolf driving\n",
      "\tgolf putting\n",
      "\thitting baseball\n",
      "\thurling (sport)\n",
      "\tjuggling soccer ball\n",
      "\tkicking field goal\n",
      "\tkicking soccer ball\n",
      "\tpassing American football (in game)\n",
      "\tpassing American football (not in game)\n",
      "\tplaying basketball\n",
      "\tplaying cricket\n",
      "\tplaying kickball\n",
      "\tplaying squash or racquetball\n",
      "\tplaying tennis\n",
      "\tplaying volleyball\n",
      "\tshooting basketball\n",
      "\tshooting goal (soccer)\n",
      "\tshot put\n",
      "golf\n",
      "\tgolf chipping\n",
      "\tgolf driving\n",
      "\tgolf putting\n",
      "gymnastics\n",
      "\tbouncing on trampoline\n",
      "\tcartwheeling\n",
      "\tgymnastics tumbling\n",
      "\tsomersaulting\n",
      "\tvault\n",
      "heights\n",
      "\tabseiling\n",
      "\tbungee jumping\n",
      "\tclimbing a rope\n",
      "\tclimbing ladder\n",
      "\tclimbing tree\n",
      "\tdiving cliff\n",
      "\tice climbing\n",
      "\tjumping into pool\n",
      "\tparagliding\n",
      "\trock climbing\n",
      "\tskydiving\n",
      "\tslacklining\n",
      "\tspringboard diving\n",
      "\tswinging on something\n",
      "\ttrapezing\n",
      "martial arts\n",
      "\tarm wrestling\n",
      "\tcapoeira\n",
      "\tdrop kicking\n",
      "\thigh kick\n",
      "\tpunching bag\n",
      "\tpunching person (boxing)\n",
      "\tside kick\n",
      "\tsword fighting\n",
      "\ttai chi\n",
      "\twrestling\n",
      "racquet + bat sports\n",
      "\tcatching or throwing baseball\n",
      "\tcatching or throwing softball\n",
      "\thitting baseball\n",
      "\thurling (sport)\n",
      "\tplaying badminton\n",
      "\tplaying cricket\n",
      "\tplaying squash or racquetball\n",
      "\tplaying tennis\n",
      "snow + ice\n",
      "\tbiking through snow\n",
      "\tbobsledding\n",
      "\thockey stop\n",
      "\tice climbing\n",
      "\tice fishing\n",
      "\tice skating\n",
      "\tmaking snowman\n",
      "\tplaying ice hockey\n",
      "\tshoveling snow\n",
      "\tski jumping\n",
      "\tskiing (not slalom or crosscountry)\n",
      "\tskiing crosscountry\n",
      "\tskiing slalom\n",
      "\tsled dog racing\n",
      "\tsnowboarding\n",
      "\tsnowkiting\n",
      "\tsnowmobiling\n",
      "\ttobogganing\n",
      "swimming\n",
      "\tswimming backstroke\n",
      "\tswimming breast stroke\n",
      "\tswimming butterfly stroke\n",
      "mobility - water\n",
      "\tcrossing river\n",
      "\tdiving cliff\n",
      "\tjumping into pool\n",
      "\tscuba diving\n",
      "\tsnorkeling\n",
      "\tspringboard diving\n",
      "\tswimming backstroke\n",
      "\tswimming breast stroke\n",
      "\tswimming butterfly stroke\n",
      "\twater sliding\n",
      "water sports\n",
      "\tcanoeing or kayaking\n",
      "\tjetskiing\n",
      "\tkitesurfing\n",
      "\tparasailing\n",
      "\tsailing\n",
      "\tsurfing water\n",
      "\twater skiing\n",
      "\twindsurfing\n",
      "custom\n",
      "\tpull ups\n",
      "\tbench pressing\n",
      "\tyoga\n",
      "\tsnatch weight lifting\n",
      "\tfront raises\n",
      "\tsquat\n",
      "\texercising arm\n",
      "\tdeadlifting\n",
      "\tlunge\n",
      "\tsitup\n",
      "\tdoing aerobics\n",
      "\tclean and jerk\n",
      "\tpush up\n",
      "\texercising with an exercise ball\n",
      "Sport activities labels: 134\n"
     ]
    }
   ],
   "source": [
    "from utils.kinetics import kinetics\n",
    "categories = kinetics.categories()\n",
    "classes = kinetics.classes()\n",
    "sports = kinetics.sport_categories()\n",
    "\n",
    "count = 0\n",
    "for key in categories.keys():\n",
    "    if key in sports:\n",
    "        print(key)\n",
    "        for label in categories[key]:\n",
    "            count+=1\n",
    "            print(\"\\t{}\".format(label))\n",
    "print(f'Sport activities labels: {count}')      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for activity granularity:** the trend with activity recognition is getting even more granular. For example, in golf Kinetics 400 classifies chipping, driving and putting. For swimming: backstroke, breast stroke, butterfly etc.\n",
    "\n",
    "So, from several video classification models available in torchvision pretrained on Kinetics 400 dataset, we should be able to get 130+ sports related actions classified. To quick start our development, we will jump start action recognition with pre-trained models available in PyTorch torchvision. Let's start by importing the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models    \n",
    "\n",
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, getting an appropriate model trained on Kinetics 400. Currently, PyTorch supports three models out of the box: ResNet 3D, ResNet Mixed Convolution and ResNet (2+1)D. I instantiated ResNet 3D (r3d\\_18) and commented out two other models. The important thing of course is pretrained=True flag, that saves us downloading all the videos for Kinetics 400 dataset to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): R2Plus1dStem(\n",
       "    (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.video.r3d_18(pretrained=True) \n",
    "#model = models.video.mc3_18(pretrained=True) \n",
    "model = models.video.r2plus1d_18(pretrained=True)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31505325\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to PyTorch magic, the pretrained model gets downloaded automatically, a huge time saver! Training on Kinetics 400 dataset requires a massive number of videos downloaded. So, having a pretrained video classification model in PyTorch is a great starter for a practical sport data scientist.\n",
    "\n",
    "**Practical Tip:** Having a pretrained model for Kinetics in PyTorch torchvision offers a big advantage from the practical standpoint. Downloading datasets and videos to train video classification models takes a lot of space and compute power!\n",
    "\n",
    "Next, we need to do normalization. For Kinetics, height and width are normalized to 112, and the mean = [0.43216, 0.394666, 0.37645] and std = [0.22803, 0.22145, 0.216989]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization: Kinetics 400\n",
    "\n",
    "mean = [0.43216, 0.394666, 0.37645]  \n",
    "std = [0.22803, 0.22145, 0.216989] \n",
    "\n",
    "def normalize(video): \n",
    "    return video.permute(3, 0, 1, 2).to(torch.float32) / 255\n",
    "\n",
    "def resize(video, size): \n",
    "    return torch.nn.functional.interpolate(video, size=size, scale_factor=None, mode='bilinear', align_corners=False)\n",
    "\n",
    "def crop(video, output_size): \n",
    "    # center crop    \n",
    "    h, w = video.shape[-2:] \n",
    "    th, tw = output_size \n",
    "    i = int(round((h - th) / 2.)) \n",
    "    j = int(round((w - tw) / 2.)) \n",
    "    return video[..., i:(i + th), j:(j + tw)]\n",
    "\n",
    "def normalize_base(video, mean, std): \n",
    "    shape = (-1,) + (1,) * (video.dim() - 1) \n",
    "    mean = torch.as_tensor(mean).reshape(shape) \n",
    "    std = torch.as_tensor(std).reshape(shape) \n",
    "    return (video - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use torchvision.io method to read a video file and show shape and some other useful information about the video we use as a source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255, 720, 1280, 3]) torch.Size([2, 407552]) {'video_fps': 29.97002997002997, 'audio_fps': 48000}\n"
     ]
    }
   ],
   "source": [
    "import torchvision.io \n",
    "video_file = 'media/surfing_cutback.mp4'\n",
    "video, audio, info = torchvision.io.read_video(video_file, pts_unit='sec') \n",
    "print(video.shape, audio.shape, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the original video has 255 frames and 720p resolution, that is relatively large for our model. We need to normalize the video before giving it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames 3, size 255 112\n"
     ]
    }
   ],
   "source": [
    "video = normalize(video) \n",
    "video = resize(video,(128, 171)) \n",
    "video = crop(video,(112, 112)) \n",
    "video = normalize_base(video, mean=mean, std=std)\n",
    "shape = video.shape\n",
    "print(f'frames {shape[0]}, size {shape[1]} {shape[2]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! After normalization, the video is much smaller: only 255x112 and it contains only 3 frames. If you have a GPU enabled device with CUDA, you can accelerate the process by moving both model and video tensor to the CUDA enabled device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make use of accelerated CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    video = video.cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the magic of applying our pretrained model and giving it a surfer video. The result is an array of scored classes (activities). We can print the best score, which is the number of the class in the list of activities of Kinetics dataset. This may take some time if you have a CPU only, depending on your environment, so be patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the video\n",
    "score = model(video.unsqueeze(0)) \n",
    "# get prediction with max score\n",
    "prediction = score.argmax() \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting index is not very meaningful, so let's get back the actual class name it represents, by using our utility script. And it turns out to be 'surfing water', the class Kinetics model was trained with to detect surfing action. Our predicted result is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kinetics import kinetics\n",
    "classes = kinetics.classes()\n",
    "print(classes[prediction.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we used a custom video file from a consumer grade 720p resolution video camera. We used a PyTorch pretrained model trained on Kinetics dataset for video classification of 400 activities, of which more than more than a hundred are sports related. We normalized the video and classified surfing correctly on the video.\n",
    "\n",
    "## Loading videos for classification\n",
    "\n",
    "PyTorch includes a number of modules simplifying video classification. In the previous project you already explored an introduction to video classification, based on a pretrained models, included in torchvision. We also used torchvision.io.read\\_video method to load videos in a convenient structure of tensors that include video frames, audio and relevant video information. In the following project we'll take it further and will do some practical video loading and model training, as well as transfer learning for video classification.\n",
    "\n",
    "### Project 9-2. Loading videos for classifier training\n",
    "\n",
    "In this project I'll show you how to use video dataset modules, such as Kinetics400 and DataLoader to visualize videos and prepare them for training. Kinetics folder structure follows a common convention that includes train/test/validation folders and videos split into classes of actions we need to recognize. Since datasets, such as Kinetics are based on indexed online videos, there're many scripts out there that simplify loading videos for training and structuring them in folders. For now, we'll define the base directory of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.kinetics import Kinetics400\n",
    "from torchvision.datasets.samplers import DistributedSampler, UniformClipSampler, RandomClipSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "Path.ls = lambda x: [o.name for o in x.iterdir()]\n",
    "from torchvision.io.video import read_video\n",
    "from functools import partial as partial\n",
    "read_video = partial(read_video, pts_unit='sec')\n",
    "torchvision.io.read_video = partial(torchvision.io.read_video, pts_unit = 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('data/kinetics400/')\n",
    "data_dir = base_dir/'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree {data_dir/'train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, as part of torchvision.datasets, PyTorch includes Kinetics400 dataset that serves as a cookie cutter for our project. Internally, video datasets use VideoClips object to store video clips data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.49s/it]\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although you can and should take advantage of the multiprocessing nature of datasets, especially in the production environment, on some systems you may get an error, num_workers = 0 makes sure you use dataset single threaded.\n",
    "\n",
    "According to this constructor above, each video clip loaded with our dataset should be a 4D tensor with the shape (frames, height, width, channels), in our case 32 frames, RGB video, note that Kinetics doesn't require all clips to be of the same height/width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 480, 272, 3])\n"
     ]
    }
   ],
   "source": [
    "print((data[0][0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing dataset\n",
    "\n",
    "Sometimes, it may be handy to visualize the entire dataset catalog as a table, summarizing the number of frames. The helper function to_dataframe loads the entire video catalog into Pandas DataFrame and displays the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 35271\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>frames</th>\n",
       "      <th>fps</th>\n",
       "      <th>clips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\Y...</td>\n",
       "      <td>250</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\Z...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\_...</td>\n",
       "      <td>178</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\a...</td>\n",
       "      <td>119</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\b...</td>\n",
       "      <td>250</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filepath  frames       fps  \\\n",
       "0    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "1    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  29.97003   \n",
       "2    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  29.97003   \n",
       "3    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "4    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "..                                                 ...     ...       ...   \n",
       "145  data\\kinetics400\\dataset\\train\\surfing_water\\Y...     250  25.00000   \n",
       "146  data\\kinetics400\\dataset\\train\\surfing_water\\Z...     300  29.97003   \n",
       "147  data\\kinetics400\\dataset\\train\\surfing_water\\_...     178  29.97003   \n",
       "148  data\\kinetics400\\dataset\\train\\surfing_water\\a...     119  15.00000   \n",
       "149  data\\kinetics400\\dataset\\train\\surfing_water\\b...     250  25.00000   \n",
       "\n",
       "     clips  \n",
       "0      269  \n",
       "1      269  \n",
       "2      269  \n",
       "3      269  \n",
       "4      269  \n",
       "..     ...  \n",
       "145    219  \n",
       "146    269  \n",
       "147    147  \n",
       "148     88  \n",
       "149    219  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.video_classification.helpers import to_dataframe\n",
    "\n",
    "to_dataframe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we want to display the size of a video in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 35271\n"
     ]
    }
   ],
   "source": [
    "VIDEO_NUMBER = 130\n",
    "video_table = to_dataframe(data)\n",
    "video_info = video_table['filepath'][VIDEO_NUMBER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With notebook IPython.display Video helper we can also show the video embedded in the notebook, but keep in mind that setting embed=True while displaying the video may significantly increase the size of your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data\\kinetics400\\dataset\\train\\surfing_water\\PGHcKxhh5y8.mp4\" controls  width=\"400\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "Video(video_info, width=400, embed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of embedding the video, it may be sufficient to just visualize the first and last frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clip_start_end(f):\n",
    "    last = len(f)\n",
    "    plt.imshow(f[0])\n",
    "    plt.title(f'frame: 1')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(f[last-1])\n",
    "    plt.title(f'frame: {last}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_clip_start_end(data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video normalization\n",
    "\n",
    "As with most of the data, before training our model, video needs to be normalized for video classification models included in torchvision. This involves getting image data in the range \\[0,1\\] and normalizing with standard deviation and the mean provided with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.video_classification.transforms as T\n",
    "\n",
    "t = torchvision.transforms.Compose([\n",
    "        T.ToFloatTensorInZeroOne(),\n",
    "        T.Resize((128, 171)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
    "                            std=[0.22803, 0.22145, 0.216989]),\n",
    "        T.RandomCrop((112, 112))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined the transform, we can pass it to the Kinetics400 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader class in PyTorch provides many useful features and makes it easy to use from Python, including: iterable datasets, automatic batching, memory pinning, sampling and data loading order customization etc.\n",
    "\n",
    "## Finding learning rate\n",
    "\n",
    "> *Never let formal education get in the way of your learning.\\\n",
    "> *--Mark Twain\n",
    "\n",
    "Learning rate, as a hyperparameter for training neural networks is important: if you make learning rate too small, the model will likely converge too slowly.\n",
    "\n",
    "**Mysterious constant:** The so-called Karpathy constant defines the best learning rate for Adam as 3e-4. The author of the famous tweet in data science, Andrej himself in the response to his own tweet says that this was a joke. Nevertheless, the constant made it to Urban Dictionary and many data science blogs.\n",
    "\n",
    "We will not take this for granted of course and will use sound theory to find the best learning rate. In practice, a large learning rate may fail to reach model convergence. As an illustration, notice that by making learning rate too large for gradient descent, the model will never reach its minimum.\n",
    "\n",
    "![](images/ch9/fig_9-7.png)\n",
    "\n",
    "To deal with this problem, a paper by Leslie N. Smith [https://arxiv.org/pdf/1506.01186.pdf] was published that proposed a method to optimize finding learning rates. As a result, many frameworks, including fastai and PyTorch now include learning rate finder module. For PyTorch, you can use torch_lr_finder module by installing it with pip install torch-lr-finder and then use it in the code with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dataset ready for learning rate finder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 112, 112]), torch.Size([4]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.video_classification.first_clip_sampler import FirstClipSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # remove audio from the batch\n",
    "    batch = [(d[0], d[2]) for d in batch]\n",
    "    return default_collate(batch)\n",
    "\n",
    "train_sampler = FirstClipSampler(train_data.video_clips, 2)\n",
    "train_dl = DataLoader(train_data, batch_size=4, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True)\n",
    "x,y = next(iter(train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
    "# if you are getting memory problems running this, \n",
    "# try reducing DataLoader batch_size above to 16 or even 4\n",
    "lr_finder = LRFinder(model, optimizer, criterion,device=device)\n",
    "lr_finder.range_test(train_dl, end_lr=10, num_iter=90)\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of video action recognition, with the size of the data and large differences in training times for video data, it is recommended to use a proper learning rate, which is often in the middle of the descending loss curve. The module plots the loss curve, and the optimal learning rate from the chart below is somewhere near value lr = 1e-2:\n",
    "\n",
    "![](images/ch9/fig_9-8.png)\n",
    "\n",
    "Optimal learning rate is found around the middle of descending loss curve, on this figure around 10\\^-2.\n",
    "\n",
    "## Training the model\n",
    "\n",
    "Training the model for video action recognition in PyTorch follows the same principles as for image classifier, but since video classification functionality is relatively new in PyTorch, it's worth including a small example in this chapter.\n",
    "\n",
    "### Project 9-3. Video Recognition Model Training\n",
    "\n",
    "To start, let's create two datasets, for training and validation, based on built-in Kinetics object. The idea here is to take advantage of built-in objects that PyTorch offers. I use the same normalizing video transformation T, already used in previous examples. On some systems you can get a significant speed improvement if you set num_workers > 0 , but on my system I had to be conservative, so I keep it at zero (basically, it means don't take advantage of parallelization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "valid_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'valid',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows using familiar DataLoaders with video data, and for video data PyTorch includes VideoClips class used for enumerating clips in the video and also sampling clips in the video while loading. FirstClipSampler in the below example used video\\_clips property from the dataset to sample a specified number of clips in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = FirstClipSampler(train_data.video_clips, 2)\n",
    "train_dl = DataLoader(train_data, \n",
    "                      batch_size=4, \n",
    "                      sampler=train_sampler, \n",
    "                      collate_fn=collate_fn, \n",
    "                      pin_memory=True)\n",
    "valid_sampler = FirstClipSampler(valid_data.video_clips, 2)\n",
    "valid_dl = DataLoader(valid_data, \n",
    "                      batch_size=4, \n",
    "                      sampler=valid_sampler, \n",
    "                      collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and renormalizing video data can take a really long time, so you may want to save the normalized dataset in cache directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = data_dir/'.cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "cache_dir.ls()\n",
    "\n",
    "torch.save(train_data, f'{cache_dir}/train')\n",
    "torch.save(valid_data, f'{cache_dir}/valid')\n",
    "train_data = torch.load(cache_dir/'train')\n",
    "valid_data = torch.load(cache_dir/'valid') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you initialize the model with hyper-parameters, including the learning rate obtained earlier. Note that since we'll be training the model, we instantiate it without weights (pretrained=False or omitted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = models.video.r2plus1d_18()\n",
    "model.cuda()\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=5e-1, steps_per_epoch=len(train_dl), epochs=10)\n",
    "metrics_dir = cache_dir/'train-metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss can be used for training classification problems and Adam optimizer (same as we used finding the learning rate). Next, we can train the model, in the example below I chose 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from utils.video_classification.train import train_one_epoch, evaluate\n",
    "\n",
    "start_time = time.time()\n",
    " \n",
    "for epoch in range(10):\n",
    "    train_one_epoch(model, \n",
    "                    criterion, \n",
    "                    optim, \n",
    "                    lr_scheduler, \n",
    "                    train_dl, device, \n",
    "                    epoch, print_freq=100)\n",
    "    evaluate(model, \n",
    "             criterion, \n",
    "             valid_dl, \n",
    "             device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the model weights once it's trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = './videoresnet_action.pth'\n",
    "torch.save(model.state_dict(), SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we covered practical methods and tools for video action recognition and classification. We discussed data structures for loading, normalizing and storing videos, datasets for sports action classification, such as Kinetics, and deep learning models. Using readily available pre-trained models, we can classify hundreds of sport actions and train the models to recognize new activities. For a sport data scientist, this chapter provides practical examples for deep learning, movement analysis, action recognition on any video.\n",
    "\n",
    "Although video action recognition is becoming more usable today, and made progress in thousands of classifications, we are still far from the goals of generalized action recognition. That means, as a sport data scientist, you are still left with a lot of work to apply video recognition in the field. Is this the right time to make video action recognition a part of your toolbox? With practical examples and notebooks accompanying this chapter, I think that this is the right time for coaches and sport scientists to start using these methods in everyday sport data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[*Video Course*](http://ai-learning.vhx.tv) Need a deep dive? Watch my [*video course*](http://ai-learning.vhx.tv) that complements this book with additional examples and video-walkthroughs. \n",
    "\n",
    "[*Web Site*](http://activefitness.ai) for research and supplemental materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (cuda 10.1)",
   "language": "python",
   "name": "ch9-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
