{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in the Cloud\n",
    "\n",
    "## Overview\n",
    "\n",
    "![](images/ch11/fig_11-1.png)\n",
    "\n",
    "A bulk of machine learning compute tasks today happens in the data centers. As a data scientist, you may have started your research on your local computer, playing with various models, frameworks and sets of data, but there's a good chance that when your project reaches the stage when people start using it, your experiments may need many resources that the cloud provides. The goal of this chapter is going over some examples on how to deploy your data science project to the cloud, store data, train your models and ultimately give your customers access to the predictions the model provide.\n",
    "\n",
    "## Containers\n",
    "\n",
    "![](images/ch11/fig_11-2.png)\n",
    "\n",
    "At the beginning of this book when we touched on the tools used by data scientists, you remember that we discussed virtual environments. In a virtual environment it's easy to isolate a set of tools and libraries needed by one experiment from another. Such an environment would typically include a list of libraries and dependencies making it relatively easy to replicate a set of components for your project. Containers take virtual environments even further. With containers you can package your data science experiment and models, together with all supporting components, and when you are ready, distribute everything to a data center, or make it a cloud service. Even if you don't use containers explicitly, most cloud services today are using them behind the scenes to isolate and package your resources.\n",
    "\n",
    "As a data scientist, you are familiar with various ways to get Python on your system. What's the docker way to get it up and running? This one liner runs Python in a new docker container on your system:\n",
    "\n",
    "```bash\n",
    "docker run -it python:3.7 python\n",
    "```\n",
    "\n",
    "The magic that happens here is that even if this version of Python is not installed on your system, docker will pull it from the online repository of images, create a container and start Python shell in that container:\n",
    "\n",
    "```\n",
    "Unable to find image \\'python:3.7\\' locally\n",
    "3.7: Pulling from library/python\n",
    "Status: Downloaded newer image for python:3.7\n",
    "Python 3.7.7 (default, Mar 11 2020, 00:27:03)\n",
    "GCC 8.3.0 on linux\n",
    "```\n",
    "\n",
    "Earlier in the book we used lots of Jupyter notebooks. But what if your notebook requires a set of components that you need to configure in a different environment? Conveniently, you can also run notebooks from a docker container, and the Jupyter team has provided a set of public images you can start with:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 jupyter/scipy-notebook\n",
    "```\n",
    "\n",
    "When this container starts, you should be able to connect to a fully functional notebook through the Web browser. Docker containers have become de facto standard for packaging and distributing applications. A template with a set of instructions on what defines your package is called an *image*, and you can create your own image with a *Dockerfile*. A container is essentially an instance of an image. As a practical data scientist, if you work with the cloud you may occasionally need to wrap your model into a container.\n",
    "\n",
    "Notebooks in the Cloud\n",
    "======================\n",
    "\n",
    "![](images/ch11/fig_11-3.png)\n",
    "\n",
    "Running Jupyter notebooks on your local machine is not the only way to run your data science experiments with some great (and often free to start) services available today: most major cloud vendors provide services that can get you started quickly with the notebooks. For starters some of the major notebook services available today include Microsoft Azure Notebooks, Google Colaboratory and Amazon. Beyond offering basic Jupyter notebooks, they often provide a set of tools that simplify managing machine learning workflows, getting and processing data etc.\n",
    "\n",
    "We'll start by creating a free notebook using Azure Machine Learning and connecting to the workspace environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python snippet takes advantage of the Azure Python SDK, a set of methods that simplifies working with objects in the workspace. If you want to connect to the cloud workspace from the local Jupyter notebook, you can simply export configuration config.json file and place it in the directory where you run your local notebook, then magically your Workspace.from\\_config() command will use your local configuration file to connect with your cloud environment.\n",
    "\n",
    "Data in the Cloud\n",
    "=================\n",
    "\n",
    "One of the most important advantages of developing your machine learning experiments in the cloud is the use of cloud-based storage. Storage in the cloud is a relatively inexpensive resource that you can easily scale, with added benefits of security, durability and high availability across different geographical regions and accessibility anywhere in the world from many languages and platforms.\n",
    "\n",
    "## Project: Using cloud storage for machine learning\n",
    "\n",
    "In the previous chapters, when we discussed various ways to capture data from athletes, I mentioned IMUs, inertial measurement unit that can aggregate data from several sensors: accelerometers, gyroscopes, magnetometers to provide accurate high frequency information about athlete movements. To illustrate the use of this data in the machine learning environment in the cloud, I provided a motion capture file of a high-level skier performing slalom turns. This data was captured using high-quality mocap with Xsens suit that combines multiple sensors.\n",
    "\n",
    "In addition to storing data in the cloud, as a data scientist you are likely to spend some time parsing the file and getting it into various models for training and further processing. In the example below, we'll load an output from such a set of IMUs, into the cloud-based machine learning workspace. The following code snippet takes a comma separated file containing center of mass data and creates a tabular dataset, registering it in the cloud workspace:\n",
    "\n",
    "![](images/ch11/fig_11-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "source_dir = os.getcwd()\n",
    "store_path = 'center_of_mass'\n",
    "\n",
    "datastore.upload_files(\n",
    "    files=[os.path.join(source_dir, f) for f in ['skier_center_of_mass.csv']],\n",
    "    relative_root=source_dir,\n",
    "    target_path=store_path,\n",
    "    overwrite=True)\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, store_path))\n",
    "dataset = dataset.register(workspace=workspace,\n",
    "                           name='center_of_mass',\n",
    "                           description='skier center of mass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling data in the cloud\n",
    "==========================\n",
    "\n",
    "For machine learning tasks like classification and object detection you'll often need to label data for training. In the previous chapters, when we discussed deep computer vision and classification, when you used a labeled dataset of different sport activities and a pretrained model to classify activities. If you recall, we had two classes of actions: 'tennis' and 'surfing' and we trained our model with a set of pre-classified images. We also used transfer learning from a pretrained model, which reduced our need in the number of images we supplied for the model. The task of labeling data in the cloud often needs to be done by a distributed team, with thousands of images, and the cloud comes very handy.\n",
    "\n",
    "Once the data is labeled, it can be exported as in COCO format (Common Objects in Context), the standard we used earlier in the book when we experimented with human body poses to store joints information. COCO data format is frequently used for object and keypoint detection, segmentation and captioning.\n",
    "\n",
    "![Using cloud based labeling project for activity classification](images/ch11/fig_11-6.png)\n",
    "\n",
    "## Project: Training a classification model on a labeled dataset in the cloud\n",
    "\n",
    "Let's use the dataset we just labeled in the cloud to train our activity classification model. You probably wonder at this point, where does the labeled dataset live in the cloud and how to get access to it? The following code snippet obtains the dataset from the workspace in the cloud, then loads it as pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial experiment configuration\n",
    "experiment_name = 'activity-classification'\n",
    "script_folder = 'activity-classification'\n",
    "cluster_name = \"compute-experiments\"\n",
    "model_file_name = 'activities.pkl'\n",
    "labeled_dataset_name = 'Classifying activities-2020-03-15 00:54:26'\n",
    "output_folder =  './outputs'\n",
    "local_download_folder = './download/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name=labeled_dataset_name)\n",
    "dataset_pd = dataset.to_pandas_dataframe(\n",
    "    file_handling_option=FileHandlingOption.DOWNLOAD, \n",
    "    target_path=local_download_folder, \n",
    "    overwrite_download=True)\n",
    "dataset_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although labeled dataset contains URLs to images, you also have ability to download image files locally, by using FileHandlingOption.DOWNLOAD. Once the data is labeled and exported, you can visualize it using standard Python libraries. Note that since this is a labeled dataset, each image now includes a label of the activity ('surfing' or 'tennis'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "plt.subplots_adjust(hspace=0.001)\n",
    "columns = 2\n",
    "rows = 2\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = mpimg.imread(dataset_pd.loc[i+5,'image_url'])\n",
    "    ax = fig.add_subplot(rows, columns, i)\n",
    "    ax.title.set_text(dataset_pd.loc[i+5,'label'])\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Displaying labeled images in Python](images/ch11/fig_11-7.png)\n",
    "\n",
    "In the earlier chapters, we used PyTorch's torchvision to load our dataset locally. Conveniently, our cloud-labeled dataset can be easily converted to a torchvision dataset, containing torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "pytorch_dataset = dataset.to_torchvision()\n",
    "img = pytorch_dataset[0][0]\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing for training\n",
    "======================\n",
    "\n",
    "Before we start training our model, we need to tell the cloud where the model is trained or specify a compute target: a VM or a compute cluster that satisfies the needs of your model, including GPU support and size. You can connect to an existing compute target, created with your workspace or add a new one, in this case I connect to compute target called 'compute-experiments':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': None, 'creationTime': '2020-03-12T14:08:17.927990+00:00', 'createdBy': {'userId': 'e180613e-2ad1-41cc-8aae-8d4183f7b2fd', 'userOrgId': '72f988bf-86f1-41af-91ab-2d7cd011db47'}, 'modifiedTime': '2020-03-12T14:09:04.221524+00:00', 'state': 'Running', 'vmSize': 'STANDARD_D3_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a new compute target with ComputeTarget.create() method. When you configured your local computer for machine learning, in the previous chapters you probably used Anaconda to manage virtual environments. Similarly, in the cloud you have a way to provision your compute target and the environment that your model needs, note that you can include Python packages in CondaDependencies of your environment (pretty cool, huh?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment('conda-env')\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                             'azureml-contrib-dataset',\n",
    "                                                                             'torch','torchvision',\n",
    "                                                                             'azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists deal with many frameworks and libraries to train models: PyTorch, Keras, scikit-learn, TensorFlow, Chainer etc. Most of model development falls into the same pattern: first you specify an environment to train your model, including dependencies, parameters and scripts that define your experiment and how the model is trained, then the model is trained and saved or registered in the workspace. Azure ML SDK provides two useful abstractions: one that wraps our experiments in the Experiment object, and another one, called Estimator that simplifies model training. In the following code snippet, I create an experiment and an estimator with a script named train.py we'll discuss in the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "dataset = Dataset.get_by_name(workspace, name=labeled_dataset_name)\n",
    "\n",
    "script_params = {\n",
    "    '--output-folder': output_folder,\n",
    "    '--model-file': model_file_name\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=script_folder, \n",
    "                entry_script='train.py',\n",
    "                script_params=script_params,    \n",
    "                inputs=[dataset.as_named_input('activities')],\n",
    "                compute_target=compute_target,\n",
    "                environment_definition=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training in the cloud\n",
    "===========================\n",
    "\n",
    "In Chapter 6, you used PyTorch to train a model to classify a sport activity. We used a local notebook to run our training, and our dataset was already labeled: all images were placed in the folders corresponding to the names of the classes: *surfing* or *tennis*.\n",
    "\n",
    "In this cloud-based project, we will use *activities* dataset we labeled using the cloud workflow from the previous section, and since earlier we already told the estimator where our training entry point will live, we'll place all our training code in the script train.py. Fortunately, we can reuse most of our model training code used for classification in Chapter 6, making adjustment for running it in the cloud. When the training script runs in the cloud, Run object maintains context information about our experiment environment, including input datasets we send to the model for training. You can obtain the context of the experiment by using Run.get\\_context() call, and then get our labeled activities dataset from run.input\\_datasets\\['activities'\\]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "run = Run.get_context()\n",
    "# get input dataset by name\n",
    "labeled_dataset = run.input_datasets['activities']\n",
    "\n",
    "mounted_path = tempfile.mkdtemp()\n",
    "# mount dataset onto the mounted_path of a Linux-based compute\n",
    "mount_context = labeled_dataset.mount(mounted_path)\n",
    "mount_context.start()\n",
    "print(os.listdir(mounted_path))\n",
    "print (mounted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load() method below loads images from the labeled dataset and applies transformation that ResNet model requires. Remember, that a pretrained model needs all images normalized in the same way. The model expects all images to be 224 pixels, with 3 RGB channels, and normalized using mean = \\[0.485, 0.456, 0.406\\] and standard deviation std = \\[0.229, 0.224, 0.225\\]. As the script loads images, it also performs normalization. We will also split the dataset between training and testing, like we did in Chapter 6 example when the model was trained using a local notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "f = './download/workspaceblobstore/activities'\n",
    "\n",
    "def load(f, size = .2):\n",
    "    \n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    train = datasets.ImageFolder(f, transform=t)\n",
    "    test = datasets.ImageFolder(f, transform=t)\n",
    "    n = len(train)\n",
    "    indices = list(range(n))\n",
    "    split = int(np.floor(size * n))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train,sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(test, sampler=test_sampler, batch_size=64)\n",
    "    return trainloader, testloader\n",
    "\n",
    "trainloader, testloader = load(f, .2)\n",
    "print(trainloader.dataset.classes)\n",
    "images, labels = next(iter(trainloader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like last time, we will use a pretrained ResNet model, trained with ImageNet, using transfer learning. Basically, we instruct PyTorch to avoid backpropagation by setting requires\\_grad to False. Then we replace the last fully connected layer with a Linear classifier for 2 classes of our labeled dataset, *surfing* and *tennis*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  \n",
    "    \n",
    "run = Run.get_context()\n",
    "\n",
    "# get input dataset by name\n",
    "#labeled_dataset = run.input_datasets['activities']\n",
    "#pytorch_dataset = labeled_dataset.to_torchvision()\n",
    "\n",
    "\n",
    "features = model.fc.in_features\n",
    "model.fc = nn.Linear(features, len(labels))\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "print_every = 100\n",
    "\n",
    "def train_model(epochs=3):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            i += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(total_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {total_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    return model\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "model = train_model(epochs=3)\n",
    "torch.save(model, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ch11/fig_11-7.png)\n",
    "\n",
    "Finally, we call train method and when the training is finished, our model is saved in the experiment instance's ./outputs folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "model = train_model(epochs=3)\n",
    "print('Finished training, saving model')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "torch.save(model, os.path.join(output_folder, model_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make our notebook write the whole train.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_name/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import tempfile\n",
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "def load(f, size = .2):\n",
    "    \n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    train = datasets.ImageFolder(f, transform=t)\n",
    "    test = datasets.ImageFolder(f, transform=t)\n",
    "    n = len(train)\n",
    "    indices = list(range(n))\n",
    "    split = int(np.floor(size * n))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train,sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(test, sampler=test_sampler, batch_size=64)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def get_mounting_path(labeled_dataset):\n",
    "    \n",
    "    mounted_path = tempfile.mkdtemp()\n",
    "    mount_context = labeled_dataset.mount(mounted_path)\n",
    "    mount_context.start()\n",
    "    print(os.listdir(mounted_path))\n",
    "    print (mounted_path)\n",
    "    print(os.listdir(mounted_path+'/workspaceblobstore'))\n",
    "    return mounted_path + '/workspaceblobstore/activities'\n",
    "\n",
    "def start(output_folder, model_file_name):\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    labeled_dataset = run.input_datasets['activities']\n",
    "    \n",
    "    data_path =  get_mounting_path(labeled_dataset)\n",
    "\n",
    "    trainloader, testloader = load(data_path, .2)\n",
    "    print(trainloader.dataset.classes)\n",
    "    images, labels = next(iter(trainloader))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  \n",
    "\n",
    "    features = model.fc.in_features\n",
    "    model.fc = nn.Linear(features, len(labels))\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    # train the model\n",
    "    print_every = 100\n",
    "    train_losses, test_losses = [], []\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    epochs=3\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            i += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(total_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {total_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    \n",
    "    print('Finished training')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    torch.save(model, os.path.join(output_folder, model_file_name))\n",
    "    print('Model saved:', model_file_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-folder\", default=None, type=str, dest='output_folder', required=True, help=\"Output folder for the model\")    \n",
    "    parser.add_argument(\"--model-file\", default=None, type=str, dest='model_file_name', required=True, help=\"Output model file\")\n",
    "    args = parser.parse_args()\n",
    "    if args.output_folder:\n",
    "        os.makedirs(args.output_folder, exist_ok=True)\n",
    "    output_folder = args.output_folder\n",
    "    model_file_name = args.model_file_name\n",
    "    print('Output folder:', output_folder)\n",
    "    print('Model file:', model_file_name)\n",
    "    start(output_folder, model_file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running experiments in the cloud\n",
    "================================\n",
    "\n",
    "Now, you created your compute target, experiment, training script and estimator, you can submit your experiment and wait until the model is trained! Everything we've created earlier in this chapter was a prelude to these two lines of code: our training script, the definition of our experiments and cloud environment, this is where all the action takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ch11/fig_11-9.png)\n",
    "\n",
    "**Note:** the above command may take a long time! Your compute target is first provisioned with all required dependencies before it starts the actual training.\n",
    "\n",
    "Model management\n",
    "================\n",
    "\n",
    "After you trained your model, you can register it in the cloud. A trained model is the 'brain' of your AI, and can be used from an API, a Web service, or any other endpoint to provide meaningful information to your customers. In this example, the model we trained to provide activity classification is registered as activity\\_classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities\tactivities:3\t3\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model(model_name='activities', model_path=output_folder+\"/\"+model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also download the model to your local device, this model can be loaded in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(name=output_folder+\"/\"+model_file_name, output_file_path='./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "=======\n",
    "\n",
    "Using cloud-based machine learning methods is the natural step in bringing your experiment to your customer. In this chapter we looked at some familiar tasks, like using notebooks, loading and processing data, labeling, classification and training your models. Everything you've done on your local computer with notebooks (and more!) you can do in the cloud. In this chapter you learned how to take a familiar task, like an image classification for different sport activities and create a pipeline for training it in the cloud. Some new concepts include creating an environment for your experiment, including Python packages dependencies, defining a compute target to run your training and registering your model in the cloud environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
