{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12. Machine Learning: Automating and Consuming\n",
    "\n",
    "> There\\'s a way to do it better - find it.\n",
    ">\n",
    "> Thomas A. Edison\n",
    "\n",
    "![](images/ch12/fig_12-1.PNG)\n",
    "\n",
    "Overview\n",
    "========\n",
    "\n",
    "In the previous chapter you used some of the cloud tools to label and process our data, train the models and register them in the cloud. As a researcher, you are likely to train quite a few models, changing your training script, experimenting with data, before making them available to your customers. This chapter is about making AI a high quality, automated process, that makes it easy to manage your code, publish models and consume them. You'll see the term CI/CD (Continuous Integration/Continuous Delivery) many times referring to the development cycle in machine learning, and in this chapter I'll be going over some practical examples of taking your research to the level of best practices and standards used in modern data science.\n",
    "\n",
    "Managing models\n",
    "===============\n",
    "\n",
    "In the last chapter you trained a classification model that can classify images of sport activities. As a data scientist you feel happy: your model converges, it predicts sport activities with good accuracy, and now your customer wants to use it. To make it available for your client, the model needs to be deployed, so you can give your clients something like a link to an API, and then they can use it in their own apps.\n",
    "\n",
    "# Project: Registering, Deploying and Comsuming Models in the Cloud\n",
    "\n",
    "The first step in deploying your models is registering them in the workspace, this saves them in the cloud so they can be used later from your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial experiment configuration\n",
    "experiment_name = 'activity-classification'\n",
    "script_folder = 'activity-classification'\n",
    "cluster_name = \"pipeline-cluster\"\n",
    "model_file_name = 'activities.pkl'\n",
    "labeled_dataset_name = 'Classifying activities-2020-03-15 00:54:26'\n",
    "output_folder =  'outputs'\n",
    "local_download_folder = './download/' \n",
    "env_name = 'pipeline-env'\n",
    "experiment_name = 'pipeline-experiment'\n",
    "model_name = 'activities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Workspace\n",
    "\n",
    "auth = InteractiveLoginAuthentication(tenant_id = '72f988bf-86f1-41af-91ab-2d7cd011db47')\n",
    "workspace = Workspace.from_config(auth = auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model activities_classifier\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"./models\",\n",
    "                       model_name = \"activities_classifier\",\n",
    "                       description = \"Activity Classification\",\n",
    "                       tags = {'area':'classification'},\n",
    "                       workspace = workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, referencing your models becomes super-easy, simply pass your workspace and model name and in your code, you have a reference to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities_classifier 3 {'area': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "model = Model(workspace, 'activities_classifier')\n",
    "print(model.name, model.version, model.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the path in the cloud of the model you just deployed, note that registration automatically versions your models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml-models\\\\activities_classifier\\\\3\\\\models\\\\activities.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.get_model_path('activities_classifier', _workspace=workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location of your registered model in the workspace will become important in the next steps, because you'll need to reference this model in your scoring script's initialization, when this model is loaded by your service.\n",
    "\n",
    "Creating a scoring script\n",
    "=========================\n",
    "\n",
    "> Knowledge is a treasure, but practice is the key to it.\n",
    ">\n",
    "> Lao Tzu\n",
    "\n",
    "You already created a script to train your model, another script you'll need is the scoring, or inferencing script, typically named score.py. The script is often specific to your model, in our case we use PyTorch and torchvision, but if you use a different machine learning model library, your script will have a similar structure but use methods specific to your environment and model for inference. The idea is, that the script runs in the context of a Web service or an API. The scoring script needs two methods: init() and run(). The first one, init() is executed once when the container with your model is started, and loads the model and classes into a global variable.\n",
    "\n",
    "The run() method is invoked each time your model is called to predict something. For the run method, since our model classifies an activity based on an image, the image needs to be decoded first from an HTTP request, and then transformed according to the size, mean and standard deviation that our model was trained with in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/score.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def transform(image_file):\n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "    image = Image.open(image_file)\n",
    "    image = t(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def decode_base64_to_img(base64_string):\n",
    "    base64_image = base64_string.encode('utf-8')\n",
    "    decoded_img = base64.b64decode(base64_image)\n",
    "    return BytesIO(decoded_img)\n",
    "\n",
    "def init():\n",
    "    global model, classes\n",
    "    #model_path = Model.get_model_path('activities')\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'models', 'activities.pkl')\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.eval()\n",
    "    #pkl_file = open(os.path.join(model_path,'class_names.pkl'), 'rb')\n",
    "    #classes = pickle.load(pkl_file)\n",
    "    #pkl_file.close() \n",
    "    classes = ['surfing','tennis']\n",
    "\n",
    "def run(input_data):\n",
    "    image = decode_base64_to_img(json.loads(input_data)['data'])\n",
    "    image = transform(image)\n",
    "\n",
    "    output = model(image)\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    pred_probs = softmax(model(image)).detach().numpy()[0]\n",
    "    index = torch.argmax(output, 1)\n",
    "\n",
    "    result = json.dumps({\"label\": classes[index], \"probability\": str(pred_probs[index])})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining an environment\n",
    "=======================\n",
    "\n",
    "Your inference environment defines your machine learning Web service configuration. You can use either Anaconda or pip requirements file to create your environment. For example, for Anaconda use a YAML file similar to what conda env export command generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/activities.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/activities.yml\n",
    "name: Activities-PyTorch\n",
    "dependencies:\n",
    "  - python=3.6.2\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "    - azureml-core\n",
    "    - azureml-contrib-dataset\n",
    "    - azureml-dataprep[pandas,fuse]\n",
    "    - inference-schema[numpy-support]\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you create the environment by using either from\\_pip\\_requirements or from\\_conda\\_specification method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"Activities-PyTorch\",\n",
       "    \"version\": \"2\",\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"python\": {\n",
       "        \"userManagedDependencies\": false,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"azureml-core\",\n",
       "                        \"azureml-contrib-dataset\",\n",
       "                        \"azureml-dataprep[pandas,fuse]\",\n",
       "                        \"inference-schema[numpy-support]\",\n",
       "                        \"torch\",\n",
       "                        \"torchvision\",\n",
       "                        \"pillow\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"azureml_29be90bc0029fbe93f78eab2d8a6383f\"\n",
       "        }\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"enabled\": false,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\",\n",
       "        \"baseDockerfile\": null,\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null,\n",
       "        \"arguments\": [],\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"username\": null,\n",
       "            \"password\": null\n",
       "        }\n",
       "    },\n",
       "    \"spark\": {\n",
       "        \"repositories\": [],\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true\n",
       "    },\n",
       "    \"databricks\": {\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"eggLibraries\": []\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"inferencingStackVersion\": null\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "env = Environment.from_conda_specification(name='Activities-PyTorch',file_path=script_folder+\"/activities.yml\")\n",
    "env.register(workspace=workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are in a big friendly cloud, there're many environments that you can easily reuse, just run this script to see a large number of environments that are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name AzureML-TensorFlow-2.0-CPU\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-telemetry==1.2.0\n",
      "  - azureml-train-restclients-hyperdrive==1.2.0\n",
      "  - azureml-train-core==1.2.0\n",
      "  - tensorflow==2.0\n",
      "  - horovod==0.18.1\n",
      "name: azureml_a685c8fa2729bbbf4932e75b8eb0df54\n",
      "\n",
      "Name AzureML-Chainer-5.1.0-GPU\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-telemetry==1.2.0\n",
      "  - azureml-train-restclients-hyperdrive==1.2.0\n",
      "  - azureml-train-core==1.2.0\n",
      "  - chainer==5.1.0\n",
      "  - cupy-cuda90==5.1.0\n",
      "  - mpi4py==3.0.0\n",
      "name: azureml_43ae3494b9b7666919116b4a25139bcf\n",
      "\n",
      "Name AzureML-VowpalWabbit-8.8.0\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-dataprep[fuse,pandas]\n",
      "name: azureml_eed6129d1cdd3d18a4f0f2b746ad4d83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List environments availablel in the cloud\n",
    "envs = Environment.list(workspace=workspace)\n",
    "for env in list(envs)[0:4]:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying models\n",
    "================\n",
    "\n",
    "There're many ways to deploy your models, as a local Web service, Azure Kubernetes Service (AKS), Azure Container Instances (ACI), Azure Functions and more. Each deployment type has advantages: for example, a Kubernetes based deployment is best for production level scalable deployments, while container instances is a fast and easy way to deploy.\n",
    "\n",
    "In this example our model is trained with PyTorch and saved as a Python pickle .pkl file, Keras models are often saved as HDF5 .h5 files and Tensorflow saves models as protocol buffer .pb files. Open Neural Network Exchange or ONNX is a promising standard that deals with interoperability of model formats and AI tools: the initialization function of your scoring script is responsible for loading the model.\n",
    "\n",
    "It is often easy to start with a local deployment while you're developing your model, this allows you to check for any problems in your scoring and initialization script. Let's test deploying our model on the local Web server, using port 8891 as an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "def deploy_locally(model_name, port):\n",
    "    model = Model(workspace, model_name)\n",
    "    myenv = Environment.from_conda_specification(name=\"env\", file_path=script_folder+\"/activities.yml\")\n",
    "    inference_config = InferenceConfig(entry_script=script_folder+\"/score.py\", environment=myenv)\n",
    "    deployment_config = LocalWebservice.deploy_configuration(port=port)\n",
    "    return Model.deploy(workspace, model_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service = deploy_locally('activities', 8891)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, everything that happens locally will also be happening in the cloud, with our next steps. When you call Model.deploy method, your environment specification is used to build and start a docker container, the model is copied to the container, and the scoring script you created earlier is invoked at the initialization method. Let's deploy our service to the cloud now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "service_name = 'activity-classification'\n",
    "model = Model(workspace, 'activities')\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "service = Model.deploy(workspace, service_name, [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script looks very similar to the local deployment you just did with Model.deploy() but notice that deployment configuration is created with AciWebservice instead of LocalWebservice, and instead of the port you specified cpu\\_cores and memory\\_gb as parameters to size your deployment.\n",
    "\n",
    "In the above example, you loaded a single model, but what if your model doesn't generalize well, or your API exposes multiple models? It often happens in machine learning that you need to package many models into the same service API. Generally, you can register multiple models and let your initialization script load them.\n",
    "\n",
    "Calling your model\n",
    "===================\n",
    "\n",
    "![](images/ch12/fig_12-2.png)\n",
    "\n",
    "The model is successfully deployed and is now ready for users to call it. Our model accepts an image as an argument, so we need to encode it before it is sent inside JSON, as Base64 string. In the receiving script run() of your score.py file you created earlier, the image is decoded again and then our model is called to predict the activity. You can test how our model works by calling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "\n",
    "image_path = 'download/workspaceblobstore/activities/surfing/resize-DSC04631.JPG'\n",
    "\n",
    "with open(image_path, 'rb') as file:\n",
    "    byte_content = file.read()\n",
    "    \n",
    "base64_bytes = base64.b64encode(byte_content)\n",
    "base64_string = base64_bytes.decode('utf-8')\n",
    "request = json.dumps({'data': base64_string })\n",
    "prediction = service.run(input_data=request)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to our model will return the predicted activity: surfing and the probability of the prediction. Getting back to the goal we stated at the beginning of this chapter, we need to give to our customer a simple URI link they can use in a multitude of apps. To get a link to the service you just deployed, you can use service.scoring\\_uri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model inference URI: \", service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Machine Learning Delivery\n",
    "====================================\n",
    "\n",
    ">  A pile of rocks ceases to be a rock pile when somebody contemplates it with the idea of a cathedral in mind.\n",
    ">\n",
    "> Antoine Saint-Exupery\n",
    "\n",
    "In the previous sections we stepped through the process of registering a model, creating a scoring script that we used to initialize the model and provide an inferencing endpoint for the API that we published as a Web service. It turns out that this process is highly repeatable in data science. As your data science team works on the models and data, improving the models accuracy, we need to make sure we keep track of changes, issues, new models are deployed. It's important to follow engineering best practices to make our project continuously deliver value to customers.\n",
    "\n",
    "Machine Learning Pipelines\n",
    "==========================\n",
    "\n",
    "![](images/ch12/fig_12-3.png)\n",
    "\n",
    "Machine learning workflow needs an architecture and the level of automation that applies to all stages of AI projects: from source code management, to data integration, model development, unit testing, releasing models to QA and production environments, monitoring and scaling the models. In the early stages of the project you may be dealing with Jupyter notebooks, but AI projects require a solid level of automation and process management to be successful.\n",
    "\n",
    "Source code\n",
    "-----------\n",
    "\n",
    "It all starts with the source code integration: most machine learning CI/CD frameworks integrate with Github, DevOps or other source control systems. Typically, as model scripts are checked into the source control by data scientists, the pipeline may be triggered to train, package and deploy the model. The premise of continuous delivery cycle is automating this process.\n",
    "\n",
    "Automating model delivery\n",
    "-------------------------------\n",
    "\n",
    "![](images/ch12/fig_12-4.png)\n",
    "\n",
    "To start with a continuous model training and delivery, frameworks such as Azure Python SDK offer some neat tools that make it easy to wrap the process into a repeatable set of steps, conveniently called a pipeline. If you are familiar with ETL processes dealing with data, then the concept should look very familiar to you. In fact, Machine Learning pipelines are often based on the same architecture and involve data transformation steps and repeatable processes that can be scheduled or triggered to run. For the purpose of creating the pipeline, you can re-use most scripts, such as your model training script.\n",
    "\n",
    "## Project: Creating a continuous model training pipeline\n",
    "\n",
    "Runtime environment \n",
    "--------------------\n",
    "\n",
    "Before you build the pipeline, let's create an environment with dependencies we need in our model, such as PyTorch and torchvision, and runtime configuration that will be used in the pipeline. Setting docker.enabled in the environment also ensures that the environment supports containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create environment and runtime configuration\n",
    "environment = Environment(env_name)\n",
    "environment.docker.enabled = True\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                        'azureml-contrib-dataset',\n",
    "                                                                        'torch','torchvision',\n",
    "                                                                        'azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-03-24T16:59:21.234000+00:00', 'errors': None, 'creationTime': '2020-03-24T13:33:40.051086+00:00', 'modifiedTime': '2020-03-24T13:33:56.048041+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D3_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfiguration()\n",
    "config.target = compute_target\n",
    "config.environment = environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is based on classification, and in the previous chapter you created a labelled dataset as part of the workspace. We will reference that dataset in the pipeline as input for model training. The output of our pipeline is a trained model, so I created another object for model\\_folder of type PipelineData where the model will be placed at the end of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(workspace, labeled_dataset_name)\n",
    "model_folder = PipelineData(\"model_folder\", datastore=workspace.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training step \n",
    "-----------------------\n",
    "\n",
    "The first step in the pipeline is similar to our training procedure and uses Estimator object wrapped into an EstimatorStep. This step calls train.py script, takes the input of our labeled dataset and is executed in the compute environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(source_directory=script_folder, \n",
    "                      compute_target=compute_target,\n",
    "                      environment_definition=config.environment,\n",
    "                      entry_script='train.py')\n",
    "\n",
    "train_step = EstimatorStep(name = \"Train Model Step\",\n",
    "                           estimator=estimator, \n",
    "                           estimator_entry_script_arguments=['--output-folder', model_folder, '--model-file', model_file_name],\n",
    "                           outputs=[model_folder],\n",
    "                           compute_target=compute_target,\n",
    "                           inputs=[dataset.as_named_input('activities')],\n",
    "                           allow_reuse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections, you registered the model from a Jupyter notebook. To make that registration part of continuous model delivery, we also need to create an additional script to register the model. The easiest way to do this is creating a Python script file, this script will call Model.register method to register the trained model in the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/register_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/register_model.py\n",
    "import argparse\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, dest='model_name', default=\"activities\", help='Model name.')\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder', default=\"outputs\", help='Model folder.')\n",
    "parser.add_argument('--model_file', type=str, dest='model_file', default=\"activities.pkl\", help='Model file.')\n",
    "args = parser.parse_args()\n",
    "model_name = args.model_name\n",
    "model_folder = args.model_folder\n",
    "model_file = args.model_file\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Model folder:\",model_folder)\n",
    "print(\"Model file:\",model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_name = model_name,\n",
    "               model_path = model_folder+\"/\"+model_file)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining deployment step\n",
    "------------------------\n",
    "\n",
    "To run this script, you can add another step to the pipeline, using a generic PythonScriptStep step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = PythonScriptStep(name = \"Register Model Step\",\n",
    "                                source_directory = script_folder,\n",
    "                                script_name = \"register_model.py\",\n",
    "                                arguments = ['--model_name', model_name, '--model_folder', model_folder, '--model_file', model_file_name],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = compute_target,\n",
    "                                runconfig = config,\n",
    "                                allow_reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps created\n",
      "Pipeline created\n"
     ]
    }
   ],
   "source": [
    "steps = [train_step, register_step]\n",
    "print(\"Steps created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=workspace, steps=steps)\n",
    "print (\"Pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pipeline\n",
    "--------------------\n",
    "\n",
    "Everything you've done so far was defining the pipeline and preparing to run it: the longest running part in the workflow is also the shortest in terms of the code. To run your workflow, create a new experiment and submit your pipeline to it, this may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline_run = Experiment(workspace, experiment_name).submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "=======\n",
    "\n",
    "In this chapter I evolved our sport classification data science experiment from simple Jupyter notebooks to the level of a professional grade project that follows best engineering practices with continuous model training and deployment. I started with a practical example of deploying the model trained in the previous chapter to the cloud and explaining how to register and manage it. Then I created a scoring script including methods for initialization and inference. I demonstrated how to define our experimental environment, including compute target and dependencies, such as ML framework that will be used in a container running our model. Then I showed how to consume the model via a Web service endpoint: both locally and from the cloud. To create a complete CI/CD automated model training and delivery, I also wrapped these steps into a machine learning pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
